import { BLOCK_SIZE, BLOCK_SIZE_MASK, createTarDecoder, createTarHeader, createTarOptionsTransformer, createTarPacker, encoder, generatePax, streamToBuffer } from "../web-CFnzlt0L.js";
import { createReadStream, createWriteStream } from "node:fs";
import * as fs from "node:fs/promises";
import { lstat, readdir, readlink, stat } from "node:fs/promises";
import * as path from "node:path";
import { join } from "node:path";
import { Readable, Writable } from "node:stream";
import { pipeline } from "node:stream/promises";

//#region src/fs/archive.ts
function packTarSources(sources) {
	const { readable, controller } = createTarPacker();
	(async () => {
		for (const source of sources) {
			const targetPath = source.target.replace(/\\/g, "/");
			switch (source.type) {
				case "file":
					await addFileToPacker(controller, source.source, targetPath);
					break;
				case "directory":
					await addDirectoryToPacker(controller, source.source, targetPath);
					break;
				case "content": {
					const { content, mode } = source;
					if (content instanceof Blob) {
						const entryStream = controller.add({
							name: targetPath,
							size: content.size,
							mode,
							type: "file"
						});
						await content.stream().pipeTo(entryStream);
						break;
					}
					if (content instanceof ReadableStream) {
						const chunks = [];
						for await (const chunk of Readable.fromWeb(content)) chunks.push(Buffer.isBuffer(chunk) ? chunk : Buffer.from(chunk));
						const buffer = Buffer.concat(chunks);
						const writer$1 = controller.add({
							name: targetPath,
							size: buffer.length,
							mode,
							type: "file"
						}).getWriter();
						await writer$1.write(buffer);
						await writer$1.close();
						break;
					}
					let data;
					if (content === null || content === void 0) data = new Uint8Array(0);
					else if (content instanceof Uint8Array) data = content;
					else if (content instanceof ArrayBuffer) data = new Uint8Array(content);
					else if (typeof content === "string") data = encoder.encode(content);
					else throw new TypeError(`Unsupported content type for entry "${targetPath}". Expected string, Uint8Array, ArrayBuffer, Blob, ReadableStream, or undefined.`);
					const writer = controller.add({
						name: targetPath,
						size: data.length,
						mode,
						type: "file"
					}).getWriter();
					await writer.write(data);
					await writer.close();
					break;
				}
			}
		}
	})().then(() => controller.finalize()).catch((err) => controller.error(err));
	return Readable.fromWeb(readable);
}
async function addFileToPacker(controller, sourcePath, targetPath) {
	const stat$1 = await fs.stat(sourcePath);
	const entryStream = controller.add({
		name: targetPath,
		size: stat$1.size,
		mode: stat$1.mode,
		mtime: stat$1.mtime,
		type: "file"
	});
	await pipeline(createReadStream(sourcePath), Writable.fromWeb(entryStream));
}
async function addDirectoryToPacker(controller, sourcePath, targetPathInArchive) {
	const sourceStat = await fs.stat(sourcePath);
	controller.add({
		name: `${targetPathInArchive}/`,
		type: "directory",
		mode: sourceStat.mode,
		mtime: sourceStat.mtime,
		size: 0
	}).close();
	const dirents = await fs.readdir(sourcePath, { withFileTypes: true });
	for (const dirent of dirents) {
		const fullSourcePath = path.join(sourcePath, dirent.name);
		const archiveEntryPath = path.join(targetPathInArchive, dirent.name).replace(/\\/g, "/");
		if (dirent.isDirectory()) await addDirectoryToPacker(controller, fullSourcePath, archiveEntryPath);
		else if (dirent.isFile()) await addFileToPacker(controller, fullSourcePath, archiveEntryPath);
	}
}

//#endregion
//#region src/fs/pack.ts
function packTar(directoryPath, options = {}) {
	const { dereference, filter, map } = options;
	const seenInodes = /* @__PURE__ */ new Map();
	const getStat = dereference ? stat : lstat;
	async function* walk(currentPath) {
		const fullPath = join(directoryPath, currentPath);
		const stat$1 = await getStat(fullPath);
		if (filter?.(fullPath, stat$1) === false) return;
		let header = {
			name: currentPath.replace(/\\/g, "/"),
			mode: stat$1.mode,
			mtime: stat$1.mtime,
			uid: stat$1.uid,
			gid: stat$1.gid,
			size: 0,
			type: "file"
		};
		if (stat$1.isFile()) {
			header.size = stat$1.size;
			if (stat$1.nlink > 1) {
				const linkTarget = seenInodes.get(stat$1.ino);
				if (linkTarget) {
					header.type = "link";
					header.linkname = linkTarget;
					header.size = 0;
				} else seenInodes.set(stat$1.ino, header.name);
			}
		} else if (stat$1.isDirectory()) {
			header.type = "directory";
			if (!header.name.endsWith("/")) header.name += "/";
		} else if (stat$1.isSymbolicLink()) {
			header.type = "symlink";
			header.linkname = await readlink(fullPath);
		}
		header = map?.(header) ?? header;
		const pax = generatePax(header);
		if (pax) {
			yield pax.paxHeader;
			yield pax.paxBody;
			const padding = -pax.paxBody.length & BLOCK_SIZE_MASK;
			if (padding > 0) yield Buffer.alloc(padding);
		}
		yield createTarHeader(header);
		if (header.type === "file" && header.size > 0) {
			yield* createReadStream(fullPath);
			const padding = -header.size & BLOCK_SIZE_MASK;
			if (padding > 0) yield Buffer.alloc(padding);
		}
		if (stat$1.isDirectory()) {
			const dirents = await readdir(fullPath, { withFileTypes: true });
			for (const dirent of dirents) yield* walk(join(currentPath, dirent.name));
		}
	}
	return Readable.from((async function* () {
		const topLevelDirents = await readdir(directoryPath);
		for (const dirent of topLevelDirents) yield* walk(dirent);
		yield Buffer.alloc(BLOCK_SIZE * 2);
	})());
}

//#endregion
//#region src/fs/path.ts
const unicodeCache = /* @__PURE__ */ new Map();
const MAX_CACHE_SIZE = 1e4;
const normalizeUnicode = (s) => {
	let result = unicodeCache.get(s);
	if (result !== void 0) unicodeCache.delete(s);
	result = result ?? s.normalize("NFD");
	unicodeCache.set(s, result);
	const overflow = unicodeCache.size - MAX_CACHE_SIZE;
	if (overflow > MAX_CACHE_SIZE / 10) {
		const keys = unicodeCache.keys();
		for (let i = 0; i < overflow; i++) unicodeCache.delete(keys.next().value);
	}
	return result;
};
async function validatePath(currentPath, root, cache) {
	const normalizedPath = normalizeUnicode(currentPath);
	if (normalizedPath === root || cache.has(normalizedPath)) return;
	const relativePath = path.relative(root, normalizedPath);
	if (!relativePath) return;
	const components = relativePath.split(path.sep);
	let current = root;
	for (const component of components) {
		current = path.join(current, component);
		if (cache.has(current)) continue;
		let stat$1;
		try {
			stat$1 = await fs.lstat(current);
		} catch (err) {
			if (err instanceof Error && "code" in err && (err.code === "ENOENT" || err.code === "EPERM")) {
				cache.add(current);
				continue;
			}
			throw err;
		}
		if (stat$1.isDirectory()) {
			cache.add(current);
			continue;
		}
		if (stat$1.isSymbolicLink()) {
			const realPath = await fs.realpath(current);
			validateBounds(realPath, root, `Path traversal attempt detected: symlink "${current}" points outside the extraction directory.`);
			cache.add(current);
			continue;
		}
		throw new Error(`Path traversal attempt detected: "${current}" is not a valid directory component.`);
	}
}
function validateBounds(targetPath, destDir, errorMessage) {
	const normalizedTarget = normalizeUnicode(targetPath);
	if (!(normalizedTarget === destDir || normalizedTarget.startsWith(destDir + path.sep))) throw new Error(errorMessage);
}

//#endregion
//#region src/fs/unpack.ts
function unpackTar(directoryPath, options = {}) {
	const { readable, writable: webWritable } = new TransformStream();
	const entryStream = readable.pipeThrough(createTarDecoder(options)).pipeThrough(createTarOptionsTransformer(options));
	const webWriter = webWritable.getWriter();
	let isWriterClosed = false;
	let isProcessingComplete = false;
	const processingPromise = (async () => {
		const resolvedDestDir = normalizeUnicode(path.resolve(directoryPath));
		const validatedDirs = new Set([resolvedDestDir]);
		await fs.mkdir(resolvedDestDir, { recursive: true });
		const reader = entryStream.getReader();
		try {
			const maxDepth = options.maxDepth ?? 1024;
			while (true) {
				const { done, value: entry } = await reader.read();
				if (done) break;
				const { header } = entry;
				const normalizedName = normalizeUnicode(header.name);
				if (maxDepth !== Infinity) {
					const depth = normalizedName.split("/").length;
					if (depth > maxDepth) throw new Error(`Path depth of entry "${header.name}" (${depth}) exceeds the maximum allowed depth of ${maxDepth}.`);
				}
				if (path.isAbsolute(normalizedName)) throw new Error(`Path traversal attempt detected for entry "${header.name}".`);
				const outPath = path.join(resolvedDestDir, normalizedName);
				validateBounds(outPath, resolvedDestDir, `Path traversal attempt detected for entry "${header.name}".`);
				const parentDir = path.dirname(outPath);
				await validatePath(parentDir, resolvedDestDir, validatedDirs);
				await fs.mkdir(parentDir, { recursive: true });
				switch (header.type) {
					case "directory": {
						const mode = options.dmode ?? header.mode;
						await fs.mkdir(outPath, {
							recursive: true,
							mode
						});
						validatedDirs.add(outPath);
						break;
					}
					case "file":
						if (header.size <= 32 * 1024) await fs.writeFile(outPath, await streamToBuffer(entry.body), { mode: options.fmode ?? header.mode });
						else await pipeline(Readable.fromWeb(entry.body), createWriteStream(outPath, { mode: options.fmode ?? header.mode }));
						break;
					case "symlink":
						if (!header.linkname) break;
						if (options.validateSymlinks ?? true) {
							const symlinkDir = path.dirname(outPath);
							const resolvedTarget = path.resolve(symlinkDir, header.linkname);
							validateBounds(resolvedTarget, resolvedDestDir, `Symlink target "${header.linkname}" points outside the extraction directory.`);
						}
						await fs.symlink(header.linkname, outPath);
						if (process.platform === "win32") {
							validatedDirs.clear();
							validatedDirs.add(resolvedDestDir);
						} else validatedDirs.delete(outPath);
						break;
					case "link": {
						if (!header.linkname) break;
						const normalizedLinkname = normalizeUnicode(header.linkname);
						if (path.isAbsolute(normalizedLinkname)) throw new Error(`Hardlink target "${header.linkname}" points outside the extraction directory.`);
						const resolvedLinkTarget = path.resolve(resolvedDestDir, normalizedLinkname);
						validateBounds(resolvedLinkTarget, resolvedDestDir, `Hardlink target "${header.linkname}" points outside the extraction directory.`);
						await validatePath(path.dirname(resolvedLinkTarget), resolvedDestDir, validatedDirs);
						await fs.link(resolvedLinkTarget, outPath);
						break;
					}
					default:
						await entry.body.cancel();
						break;
				}
				if (header.mtime) try {
					await (header.type === "symlink" ? fs.lutimes : fs.utimes)(outPath, header.mtime, header.mtime);
				} catch {}
			}
		} finally {
			isProcessingComplete = true;
			reader.releaseLock();
		}
	})().catch((err) => {
		isProcessingComplete = true;
		throw err;
	});
	return new Writable({
		async write(chunk, _encoding, callback) {
			if (isWriterClosed || isProcessingComplete || webWriter.desiredSize === null) return callback();
			try {
				await webWriter.write(chunk);
				callback();
			} catch (err) {
				if (err instanceof TypeError && err.stack?.includes("TransformStream") && webWriter.desiredSize === null) return callback();
				callback(err);
			}
		},
		async final(callback) {
			if (isWriterClosed) return callback();
			try {
				isWriterClosed = true;
				try {
					await webWriter.close();
				} catch {}
				await processingPromise;
				callback();
			} catch (err) {
				callback(err);
			}
		},
		destroy(err, callback) {
			if (isWriterClosed) return callback(err);
			isWriterClosed = true;
			isProcessingComplete = true;
			webWriter.abort(err).catch(() => {});
			entryStream.cancel(err).catch(() => {});
			processingPromise.finally(() => {
				callback(err);
			});
		}
	});
}

//#endregion
export { packTar, packTarSources, unpackTar };